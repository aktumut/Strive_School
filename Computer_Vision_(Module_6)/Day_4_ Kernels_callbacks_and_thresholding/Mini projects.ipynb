{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Photo art app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Create a function that receives an image and converts it to a sketch so it looks similar to this:\n",
    "![Example 1](img/sketch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "window_name = \"filter\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "key = 0 \n",
    "cv2.namedWindow(window_name,cv2.WINDOW_AUTOSIZE)\n",
    "while (True):\n",
    "    ret, frame = cap.read()\n",
    "    gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray_frame, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 30, 150)\n",
    "    \n",
    "    cv2.imshow(window_name,edges)\n",
    "\n",
    "    if key == 27:\n",
    "        break\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Instagram filters app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Create a function that given a photo it applies a black and white filter\n",
    "1. Create a filter that will change color of the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def black_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    image=img.copy()\n",
    "    im_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    thresh, _ = cv2.threshold(im_gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    image = cv2.threshold(im_gray, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    cv2.imshow(\"\",image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "black_image(\"img/noisy.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Photo art app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:16:21.170497Z",
     "start_time": "2019-08-11T12:16:21.096496Z"
    },
    "hidden": true
   },
   "source": [
    "1. Change the app so it can now do the same but using your webcam to make it in real time  \n",
    "**Hint:** you can use ```cv2.VideoCapture(0)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "\n",
    "window_name = \"filter\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "key = 0 \n",
    "cv2.namedWindow(window_name,cv2.WINDOW_AUTOSIZE)\n",
    "while (True):\n",
    "    ret, frame = cap.read()\n",
    "    gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    thresh, _ = cv2.threshold(gray_frame, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    frame = cv2.threshold(gray_frame, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "\n",
    "    \n",
    "    cv2.imshow(window_name,frame)\n",
    "\n",
    "    if key == 27:\n",
    "        break\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Photo correction app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Create a funtion to clean *noise* from images\n",
    "\n",
    "\n",
    "![Noisy image](img/n.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_noise(image_path):\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    image=img.copy()\n",
    "\n",
    "    cleaned_image = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n",
    "\n",
    "    cv2.imshow(\"\",cleaned_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "clean_noise('img/noisy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array([[0.272, 0.534, 0.131],\n",
    "        [0.349, 0.686, 0.168],\n",
    "        [0.393, 0.769, 0.189]])\n",
    "\n",
    "img = cv2.imread(\"img/noisy.png\")\n",
    "image=img.copy()\n",
    "\n",
    "\n",
    "R,G,B = image.shape\n",
    "\n",
    "tb = 0.272*R + 0.534*G + 0.131*B\n",
    "tg = 0.349/R + 0.686*G + 0.168/B\n",
    "tr = 0.393/R + 0.769*G + 0.189*B\n",
    "\n",
    "\n",
    "if tb > 255: b = 255 \n",
    "else: b = tb\n",
    "if tg > 255: g = 255 \n",
    "else: g = tg\n",
    "if tr > 255:r = 255 \n",
    "else: r = tr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instagram filters app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a function that applies a sepia filter to the image, for a BGR image apply this kernel:\n",
    "        [0.272, 0.534, 0.131],\n",
    "        [0.349, 0.686, 0.168],\n",
    "        [0.393, 0.769, 0.189]\n",
    "        \n",
    "Get the BGR value of the pixel.\n",
    "Calculate tr, tg and tb using the formula\n",
    "\n",
    "tb = 0.272R + 0.534G + 0.131B\n",
    "tg = 0.349R + 0.686G + 0.168B\n",
    "tr = 0.393R + 0.769G + 0.189B\n",
    "\n",
    "Take the integer value.\n",
    "\n",
    "Set the new RGB value of the pixel as per the following condition:\n",
    "\n",
    "If tb > 255 then b = 255 else b = tb\n",
    "If tg > 255 then g = 255 else g = tg\n",
    "If tr > 255 then r = 255 else r = tr\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a color pixel with the following values\n",
    "\n",
    "A = 255\n",
    "\n",
    "R = 100\n",
    "\n",
    "G = 150\n",
    "\n",
    "B = 200\n",
    "\n",
    "Where A, R, G and B represents the Alpha, Red, Green and Blue value of the pixel.\n",
    "\n",
    "Remember! ARGB will have an integer value in the range 0 to 255.\n",
    "\n",
    "So, to convert the color pixel into sepia pixel we have to first calculate tr, tg and tb.\n",
    "\n",
    "tr = 0.393(100) + 0.769(150) + 0.189(200)\n",
    "\n",
    "tr = 192.45\n",
    "\n",
    "tr = 192 (taking integer value)\n",
    "\n",
    "Similarly,\n",
    "\n",
    "tg = 0.349(100) + 0.686(150) + 0.168(200) = 171 (taking integer value)\n",
    "\n",
    "and tb = 0.272(100) + 0.534(150) + 0.131(200) = 133 (taking integer value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Instagram filters app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Apply the sepia filter by using the cv2.transfor function\n",
    "1. Create different filters from the sepia filter so it renders different images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwriting recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a one digit number (from 0 to 9) on a paper and using your mobile phone take a picture of it\n",
    "1. Crop the image manually and pass it to a a function that will process it so it looks like a char in the MNIST dataset, make sure it is the correct size as well\n",
    "![4](img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pass it through a MNIST classifier and print the prediction on screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modify the image so now it also contains the predicted result on the image\n",
    "1. Try it with all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=False, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=False, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\ntensor([[-3.9510e-03,  3.1058e-02, -1.4872e-03,  ..., -9.3022e-03,\n         -2.4733e-02,  8.1009e-04],\n        [ 5.8741e-03,  2.1129e-02,  1.4276e-02,  ...,  1.9532e-03,\n          3.4124e-02, -2.6466e-02],\n        [-2.8529e-02,  1.9764e-02, -1.1332e-02,  ...,  2.2856e-02,\n          3.2135e-02,  2.5591e-02],\n        ...,\n        [-3.2157e-02,  1.4746e-02,  4.1487e-03,  ..., -2.6372e-02,\n          1.5287e-02,  3.5504e-02],\n        [ 7.0300e-03, -1.8390e-02, -2.3230e-02,  ...,  3.0578e-02,\n         -3.3677e-03,  3.1392e-03],\n        [-6.4514e-05,  9.2151e-03,  3.1686e-02,  ...,  5.5787e-03,\n          2.4052e-03, -2.8002e-03]], requires_grad=True)\nGradient - tensor([[ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],\n        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],\n        [-0.0017, -0.0017, -0.0017,  ..., -0.0017, -0.0017, -0.0017],\n        ...,\n        [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(16, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/5\n",
      "\tIteration: 0\t Loss: 0.0008\n",
      "\tIteration: 1860\t Loss: 1.5796\n",
      "\tIteration: 3720\t Loss: 1.5812\n",
      "Epoch: 2/5\n",
      "\tIteration: 0\t Loss: 0.0009\n",
      "\tIteration: 1860\t Loss: 1.5801\n",
      "\tIteration: 3720\t Loss: 1.5802\n",
      "Epoch: 3/5\n",
      "\tIteration: 0\t Loss: 0.0009\n",
      "\tIteration: 1860\t Loss: 1.5811\n",
      "\tIteration: 3720\t Loss: 1.5786\n",
      "Epoch: 4/5\n",
      "\tIteration: 0\t Loss: 0.0009\n",
      "\tIteration: 1860\t Loss: 1.5811\n",
      "\tIteration: 3720\t Loss: 1.5778\n",
      "Epoch: 5/5\n",
      "\tIteration: 0\t Loss: 0.0009\n",
      "\tIteration: 1860\t Loss: 1.5811\n",
      "\tIteration: 3720\t Loss: 1.5778\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "print_every = 1860\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "testing set accuracy: 86 %\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0,0\n",
    "predictions = []\n",
    "model.eval()\n",
    "\n",
    "for i,data in enumerate(testloader,0):\n",
    "    inputs, labels = data\n",
    "    inputs.resize_(inputs.size()[0], 784)\n",
    "    #print(inputs.shape)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data,1)\n",
    "    predictions.append(outputs)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "print(\"testing set accuracy: %d %%\" % (100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mnist(img):\n",
    "    img = cv2.blur(img, (11, 11))\n",
    "    _, img = cv2.threshold(img, 150, 255, cv2.THRESH_TRUNC)\n",
    "    img = cv2.bitwise_not(img)\n",
    "    kernel = np.ones((11, 11), np.uint8)\n",
    "    # img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    _, img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)\n",
    "    # kernel_er = np.ones((7, 7), np.uint8)\n",
    "    # img = cv2.erode(img, kernel_er, iterations=1)\n",
    "    img = cv2.resize(img, (28, 28), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    return img\n",
    "\n",
    "def to_tensor(arr: np.ndarray):\n",
    "    tens = torch.from_numpy(arr).float()\n",
    "    tens = F.normalize(tens)\n",
    "    tens = tens.reshape(-1 ,784)\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "three = cv2.imread(\"img/3.jpg\")\n",
    "three=to_mnist(three)\n",
    "\n",
    "three=to_tensor(three)\n",
    "with torch.no_grad():\n",
    "    logit = model.forward(three)\n",
    "ps = F.softmax(logit, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'4.5.2'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('my-project-env': virtualenvwrapper)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "737e9587d30d88c29f1321911dcbdbc8762e4df40f7563cdfd1b93bad311ab30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}