{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infinite-metadata",
   "metadata": {},
   "source": [
    "To perform sentence classification, and many other classification tasks for NLP, we need to do three main steps:\n",
    "\n",
    "- Preprocessing the data\n",
    "- Prepare the dataloader\n",
    "- Build the model\n",
    "\n",
    "Of course, all of these steps requires a lot of other steps, and also they can include many different solutions. \n",
    "\n",
    "To make you to jumpstart on this task, I will provide you a pretty clean dataset, the Amazon Reviews one, that you can extensively find online, and it's also included in the `torxchtext.datasets` module. \n",
    "\n",
    "For this example, I will use just a little part of it, to give some guidance on how to start, without actually training the whole model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-copper",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personalized-access",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:04:37.990307Z",
     "start_time": "2021-07-19T18:04:37.504622Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surrounded-forwarding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:04:40.029341Z",
     "start_time": "2021-07-19T18:04:37.996816Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AmazonReviewFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = AmazonReviewFull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "label_list=[]\n",
    "review_list=[]\n",
    "for label,review in iter(train):\n",
    "    label_list.append(label)\n",
    "    review_list.append(review)\n",
    "len(label_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"star\":label_list[:3000],\"review\":review_list[:3000]}\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listed-criticism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:04:47.209292Z",
     "start_time": "2021-07-19T18:04:47.161623Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   star                                             review\n",
       "0     3  more like funchuck Gave this to my dad for a g...\n",
       "1     5  Inspiring I hope a lot of people hear this cd....\n",
       "2     5  The best soundtrack ever to anything. I'm read...\n",
       "3     4  Chrono Cross OST The music of Yasunori Misuda ...\n",
       "4     5  Too good to be true Probably the greatest soun..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>star</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>more like funchuck Gave this to my dad for a g...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>Inspiring I hope a lot of people hear this cd....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>The best soundtrack ever to anything. I'm read...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Chrono Cross OST The music of Yasunori Misuda ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Too good to be true Probably the greatest soun...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-progressive",
   "metadata": {},
   "source": [
    "The `star`column is what we want to predict, given the text of the review. I think we are all Amazon users, and we are all aware of how many stars a rating can have, but let's just double check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dated-password",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 5, 4, 1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.star.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-labor",
   "metadata": {},
   "source": [
    "Ok, now that our data are in order, we need to preprocess them. We can take advantage of spacy for basically of the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "affiliated-concrete",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:06:56.046136Z",
     "start_time": "2021-07-19T18:06:55.450651Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-portal",
   "metadata": {},
   "source": [
    "Let's create a function that, given a sentence, it preprocess it by doing:\n",
    "- tokenization\n",
    "- removing stopwords\n",
    "- remove special characters/punctuation\n",
    "- make everything lower case\n",
    "- lemmatize it\n",
    "\n",
    "With spacy, we can do it in a very compact form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "independent-error",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:07:10.961066Z",
     "start_time": "2021-07-19T18:07:10.953004Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    params sentence: a str containing the sentence we want to preprocess\n",
    "    return the tokens list\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_.lower()  for token in doc if not token.is_punct and not token.is_stop and not token.text.isdigit() or \"not\" in token.text.lower() ]\n",
    "    #punct is for commas and questions marks like this kind of stuffs (punctioation)\n",
    "    # we used \"not\" because not can block bad reactions like \"not good\"\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interior-designation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:07:12.173755Z",
     "start_time": "2021-07-19T18:07:12.147257Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['example', 'hello', 'alessio', 'not', 'not']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "preprocessing(\"This is an example! Hello 12484654 Alessio */*/* 2121 1 2 3 not not \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-positive",
   "metadata": {},
   "source": [
    "The preprocessing phase has not finished yet. In fact, we want to create a neural network, and a neural network works with numbers. In general, computers work with numbers...\n",
    "\n",
    "So we need to use embeddings to transform a sentence into a tensor: the embeddings are usually one-dimensional, and in the following example they will have size 300, that means that if you have a sentence of 10 words (after have it preprocessed), the shape of the sentence will be $10\\times 300$. You will notice another dimension, that is the batch size. So you will train and run a model that receive as input a tensor of shape:\n",
    "\n",
    "`batch_size*length_of_the_sentence*embedding_size`.\n",
    "\n",
    "Let's do things in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "registered-incidence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:18:07.324236Z",
     "start_time": "2021-07-19T18:18:07.237931Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-mistress",
   "metadata": {},
   "source": [
    "If you are using the whole dataset, you should not need to split the dataset into train and test 'cause it should be already. If not, and if you are using any other dataset, remember to split into train and test (eventually validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mental-intellectual",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:10:10.561578Z",
     "start_time": "2021-07-19T18:10:10.553750Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df.iloc[:2000], df.iloc[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-block",
   "metadata": {},
   "source": [
    "Now we need to create a vocabulary. What does it mean? We need to keep track of all the tokens that occurs in our dataset, so that we can index them with a number instead of using a string.\n",
    "\n",
    "To do so, we take advantage of the `Counter` function from the `collections` library, and then pass to the `Vocab` class https://pytorch.org/text/stable/vocab.html. This is used to create a fast way to lookup the dictionary you have created. Don't forget to update the counter *after* applying the `preprocessing` function that you have created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "banned-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "train_iter = iter(train_df.review.values)\n",
    "for text in train_iter:\n",
    "    counter.update(preprocessing(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "original-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-weather",
   "metadata": {},
   "source": [
    "As `min_freq` I chose 1, that means that I'm considering all the terms in the vocabulary that occurs at least once. There are cases in which you want to filter out some rare words, but in this reviews dataset I don't think it's a good idea.\n",
    "\n",
    "*Tip:* I'd rather do a better preprocessing, trying to use some spellchecker to correct typos so that mispelled words would be associated to the right one. However, in reality, it's pretty hard to find a good spellchecker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-advocate",
   "metadata": {},
   "source": [
    "Let's check what the `vocab` we just created do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "manufactured-geology",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4, 96]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "text = preprocessing(\"hello world\")\n",
    "\n",
    "[vocab[x] for x in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-accent",
   "metadata": {},
   "source": [
    "It seems it trasformed each token into a number. How to get the token as a string back?\n",
    "\n",
    "We can use the `.itos` method, that stands for *index to string*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__jit_unused_properties__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__prepare_scriptable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'append_token',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_default_index',\n",
       " 'get_itos',\n",
       " 'get_parameter',\n",
       " 'get_stoi',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'insert_token',\n",
       " 'is_jitable',\n",
       " 'load_state_dict',\n",
       " 'lookup_indices',\n",
       " 'lookup_token',\n",
       " 'lookup_tokens',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_default_index',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'vocab',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "dir(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual-sunrise",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Counter' object has no attribute 'get_itos'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8df01f7b8b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_itos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/my-project-env/lib/python3.8/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mget_itos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_itos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__prepare_scriptable__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Counter' object has no attribute 'get_itos'"
     ]
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-fossil",
   "metadata": {},
   "source": [
    "These things are cool, but the most important goal of vocab for the `torchtext` library is the possibility of loading pretrained embeddings by typing `vocab.load_vectors(\"name_of_the_embeddings\")`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "satellite-bobby",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'load_vectors'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e6eed1978100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fasttext.simple.300d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/my-project-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'load_vectors'"
     ]
    }
   ],
   "source": [
    "vocab.load_vectors(\"fasttext.simple.300d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-operator",
   "metadata": {},
   "source": [
    "You don't even need to assign it to a variable, it will load and store it as an object attribute. \n",
    "\n",
    "As you have seen, embeddings are a numerical representation of a word, that have been trained in an unsupervised manner on some text corpus. They help in all the text classification problems, since they encode also some semantic information about the words, in such a way that the distance (say the cosine similarity) of two vectors with a similar meaning is less than the ones with different meanings.\n",
    "\n",
    "So if inspect them, we should get a tensor containing 300 size arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "major-running",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1181, -0.3024,  0.2944,  ..., -0.1119, -0.0891, -0.1466],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "opened-calculation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13063, 300])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-timing",
   "metadata": {},
   "source": [
    "Indeed. What's the first number? It's the amount of unique tokens we have in the training set. Instead, the second term (300) is the size of the embeddings. How can we retrieve the vector corresponding to the word \"good\"?\n",
    "\n",
    "Well we have the method `.stoi` that stands for \"string to index\" to retrieve the index associated with the string \"good\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "corrected-suite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi[\"good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-arnold",
   "metadata": {},
   "source": [
    "Actually, no need to do it, we can just do `vocab[\"good\"`\n",
    " and we'll get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "auburn-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-compilation",
   "metadata": {},
   "source": [
    "and then we can use that index to retrieve the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "gothic-stage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.9637e-01,  2.7714e-01,  2.6521e-01, -3.4600e-01, -1.0764e-01,\n",
       "         1.8982e-01, -3.8168e-02,  8.3709e-02,  1.9178e-01,  1.6162e-01,\n",
       "         1.6343e-01,  1.3317e-01, -3.8956e-01, -1.3596e-01, -1.2511e-01,\n",
       "        -1.6472e-01, -1.4022e-01,  4.1599e-02, -1.4979e-01,  1.0635e-01,\n",
       "         3.6200e-01,  1.0988e-01,  1.4841e-01,  1.1830e-01,  8.3510e-02,\n",
       "        -2.1211e-01,  7.1777e-02, -7.3148e-03,  2.2641e-01,  3.7710e-02,\n",
       "        -4.5244e-03,  7.0736e-02, -7.2897e-02,  3.0860e-01,  1.5270e-01,\n",
       "        -9.1561e-02, -3.8422e-01, -1.6947e-01,  5.8803e-02, -9.8637e-03,\n",
       "         6.0262e-02, -2.4001e-01,  1.1871e-01, -1.7887e-01, -2.3948e-01,\n",
       "        -9.4501e-02, -1.5217e-01, -6.8412e-02,  8.2164e-02,  1.8725e-01,\n",
       "         3.3745e-02, -1.3283e-01, -3.0824e-01,  1.0093e-01, -3.3814e-01,\n",
       "        -5.6273e-02, -1.6498e-01,  1.3821e-01, -5.3919e-02,  2.6901e-01,\n",
       "        -4.8584e-01, -1.4683e-01, -2.5746e-01,  2.5280e-01,  2.7210e-02,\n",
       "         1.2690e-02, -1.7246e-02, -1.8162e-01, -8.4780e-02,  1.7224e-01,\n",
       "         9.5668e-02,  1.0414e-01, -3.7922e-01,  7.1001e-02,  1.1908e-01,\n",
       "        -1.6680e-01,  1.8472e-02, -2.3527e-02,  3.0313e-01,  1.5176e-01,\n",
       "        -1.3940e-02,  1.9288e-02, -8.8467e-02, -1.5372e-01, -1.9110e-01,\n",
       "        -1.1347e-01, -4.4242e-02, -3.4148e-01, -3.5365e-02, -5.8192e-02,\n",
       "         2.1651e-01, -2.0159e-01,  9.7316e-02,  3.1088e-02, -9.8637e-02,\n",
       "        -8.6075e-02, -3.4739e-01, -1.5262e-01, -1.2138e-03,  1.8232e-01,\n",
       "        -9.6376e-04, -3.4033e-02, -1.3758e-01, -6.0251e-03,  6.3835e-02,\n",
       "         3.5858e-03,  2.3128e-01, -3.9763e-02, -1.6133e-01, -1.8829e-01,\n",
       "         4.4235e-02, -3.0484e-01, -3.6469e-02, -4.4658e-02, -2.1600e-01,\n",
       "        -4.1728e-02, -2.1421e-01,  1.1116e-01, -3.5212e-01, -1.0529e-01,\n",
       "        -2.8644e-01,  3.3235e-02,  1.8587e-01, -8.1017e-02, -3.4905e-01,\n",
       "        -3.8461e-01,  2.3764e-02, -1.6903e-01, -2.8086e-01,  8.4250e-02,\n",
       "         3.1011e-01,  5.4364e-02,  7.1137e-02,  1.8771e-01, -1.5624e-01,\n",
       "         3.0394e-01, -4.1854e-02, -9.4310e-03,  1.5564e-01, -1.0878e-01,\n",
       "         9.2710e-02,  4.3704e-01,  3.7497e-01,  1.5408e-01, -1.2918e-01,\n",
       "        -6.1866e-02, -1.3198e-01, -1.9779e-01,  6.7010e-02, -3.2875e-01,\n",
       "        -2.1097e-01, -1.4853e-01,  1.2195e-01, -7.4191e-02, -2.2386e-02,\n",
       "         4.0787e-02, -2.9910e-01,  4.2943e-01, -2.1895e-01,  1.0786e-02,\n",
       "         2.0017e-01,  3.5886e-03,  1.6625e-02, -1.6902e-01, -9.0157e-02,\n",
       "         9.3869e-02,  2.6675e-02, -8.4368e-02, -2.2489e-02,  2.5627e-01,\n",
       "        -8.1175e-02,  1.1326e-01,  1.8309e-01, -1.8123e-01, -2.3453e-01,\n",
       "        -2.6526e-01, -2.0923e-01, -4.0253e-01,  9.7483e-02, -1.1192e-01,\n",
       "        -1.7071e-01,  1.9791e-02, -4.1368e-02,  2.6174e-01, -2.2172e-01,\n",
       "        -5.8976e-02, -1.4266e-01,  4.8435e-01, -4.8311e-01,  2.1035e-01,\n",
       "         2.1803e-01,  9.0008e-02,  1.0716e-01,  1.5621e-01, -7.0703e-02,\n",
       "         4.5159e-02, -4.5658e-01,  2.6034e-02,  1.2824e-01, -1.1883e-01,\n",
       "         4.1708e-01, -1.6080e-01,  1.5933e-01, -2.7090e-01,  2.6154e-01,\n",
       "        -6.7165e-02, -4.6179e-02, -6.4885e-02,  2.4443e-01, -2.4580e-01,\n",
       "        -1.3982e-01,  1.6249e-01,  1.8879e-01, -3.5974e-01, -5.7969e-01,\n",
       "         2.2056e-03,  4.2249e-01, -2.2366e-01, -1.1156e-01, -7.1421e-02,\n",
       "         3.4599e-02, -6.2135e-02,  1.5824e-01, -1.4023e-01,  1.5168e-01,\n",
       "         3.0882e-04, -8.3735e-02,  1.3617e-01,  1.4851e-01,  2.6006e-01,\n",
       "         2.3711e-01,  2.6680e-01, -1.5794e-01,  3.0945e-02, -6.7572e-03,\n",
       "         5.9429e-02,  1.2310e-02, -1.6079e-01,  1.7189e-01,  6.2470e-02,\n",
       "         4.1477e-01,  3.5336e-01,  1.4261e-01,  9.3371e-03,  4.2961e-02,\n",
       "        -3.2431e-01,  7.2694e-02, -2.1376e-01,  1.9612e-01,  3.8756e-01,\n",
       "         1.3394e-01,  2.3658e-01,  5.4745e-02, -2.2844e-01, -8.1546e-02,\n",
       "        -5.1938e-01, -5.7666e-01,  1.6498e-02, -4.1266e-02,  6.5526e-01,\n",
       "         1.7101e-01,  1.2412e-01, -1.2365e-01,  9.8372e-02, -1.0586e-01,\n",
       "         1.9889e-02,  6.5504e-02,  1.3407e-03, -1.3806e-01, -3.9546e-01,\n",
       "         7.0392e-02,  6.7426e-02, -8.6095e-02,  9.0994e-02, -2.2239e-01,\n",
       "         6.7782e-02,  1.0339e-02, -3.9664e-02,  3.4535e-01,  3.0099e-01,\n",
       "         8.3322e-02,  1.0704e-01,  6.2821e-03, -1.5934e-01, -1.8698e-02,\n",
       "         3.2465e-01,  6.1989e-03,  1.5659e-01, -2.0168e-01, -1.9255e-01,\n",
       "        -2.4134e-01, -1.4124e-01, -2.1126e-01,  5.2775e-02, -1.0469e-01,\n",
       "        -1.2732e-01, -4.1920e-01,  4.3569e-02,  5.3318e-02, -3.5837e-01])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "sudden-dimension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "annoying-thickness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-aspect",
   "metadata": {},
   "source": [
    "What about \"nice\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "turned-december",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"nice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "royal-anger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4940,  0.4000,  0.2400, -0.1512, -0.0875,  0.3711, -0.1918,  0.2610,\n",
       "         0.3133, -0.0206, -0.1875,  0.0116,  0.0544, -0.2277,  0.0178,  0.1509,\n",
       "        -0.1158, -0.2391,  0.1476, -0.5993,  0.4263, -0.2732,  0.1811, -0.1666,\n",
       "         0.2579, -0.0722,  0.2222, -0.3234, -0.0363,  0.0015, -0.1955, -0.0208,\n",
       "        -0.0191,  0.4611,  0.2720, -0.2414, -0.0971, -0.1544,  0.1492,  0.1033,\n",
       "        -0.4095,  0.2533,  0.2096,  0.0630,  0.2702, -0.3699, -0.0760, -0.1037,\n",
       "         0.1022,  0.2548, -0.0250,  0.0291,  0.2737,  0.2041,  0.0960, -0.3495,\n",
       "         0.2727,  0.0887,  0.1010,  0.3411, -0.3515, -0.1092,  0.1397, -0.0361,\n",
       "        -0.0667, -0.0639,  0.1377, -0.3333, -0.0857,  0.1262,  0.0286, -0.1108,\n",
       "        -0.1534,  0.0137, -0.1651, -0.4551,  0.1704, -0.1705,  0.1677,  0.0466,\n",
       "         0.0512, -0.2957,  0.0971,  0.1499,  0.1507, -0.3408, -0.1480, -0.3976,\n",
       "         0.1525, -0.3452,  0.2638,  0.0131,  0.1264,  0.0492,  0.0179,  0.0911,\n",
       "        -0.5341,  0.3287,  0.2030, -0.2169, -0.4362, -0.3144, -0.3074,  0.1910,\n",
       "        -0.1620,  0.0019, -0.1184,  0.1225, -0.1226,  0.1546, -0.1473, -0.1690,\n",
       "         0.3109, -0.0280, -0.2665, -0.2664, -0.1604,  0.0227, -0.3549,  0.1674,\n",
       "         0.3738, -0.0789,  0.1285,  0.2060, -0.4044, -0.2749,  0.1675, -0.3190,\n",
       "        -0.2240,  0.1157, -0.0681, -0.0815,  0.0608,  0.1433, -0.2252,  0.1120,\n",
       "        -0.0155,  0.1784, -0.3992, -0.1163, -0.1052,  0.0220,  0.3252, -0.1663,\n",
       "         0.5106, -0.3204,  0.1055,  0.0612,  0.0922, -0.3327,  0.0182, -0.0704,\n",
       "         0.0640, -0.0911,  0.5581,  0.1347, -0.1869,  0.2902,  0.1189,  0.0627,\n",
       "         0.3853, -0.1735, -0.1979, -0.4591,  0.2049,  0.1410,  0.1469, -0.2166,\n",
       "         0.1529,  0.4424, -0.2769,  0.1914,  0.1910, -0.1971, -0.4862, -0.0084,\n",
       "         0.3531, -0.7599, -0.0963,  0.0432, -0.0337,  0.1363, -0.5863,  0.2142,\n",
       "        -0.1605,  0.0409,  0.1034, -0.1185,  0.1247,  0.0974,  0.2094,  0.0534,\n",
       "         0.0391,  0.1367,  0.0084, -0.1041, -0.5552, -0.1177, -0.0450, -0.5738,\n",
       "         0.1657, -0.0762, -0.0608, -0.1093,  0.0301,  0.2009,  0.0230, -0.1257,\n",
       "         0.1855,  0.1773,  0.0856, -0.1196,  0.3370, -0.1763, -0.5596,  0.0306,\n",
       "         0.1929,  0.1114, -0.1084, -0.3068,  0.1002, -0.0769, -0.3245,  0.0947,\n",
       "         0.2004, -0.5194, -0.0290,  0.1982, -0.2822,  0.1160,  0.3508,  0.2020,\n",
       "        -0.5618, -0.0175, -0.0829,  0.1768, -0.1293, -0.0994,  0.0146, -0.1550,\n",
       "         0.2331,  0.1190, -0.1544,  0.0716,  0.4511, -0.0653, -0.2409, -0.1754,\n",
       "         0.2610,  0.6536,  0.2650,  0.5696, -0.1121,  0.0567, -0.1176, -0.1988,\n",
       "        -0.2101,  0.1155,  0.0046,  0.2926,  0.1841,  0.0766, -0.1914,  0.0429,\n",
       "        -0.2427, -0.1322, -0.0401,  0.0696,  0.1834, -0.4802, -0.1145,  0.0969,\n",
       "        -0.1690, -0.0269, -0.2559, -0.1594,  0.0850,  0.0154,  0.1325, -0.1189,\n",
       "         0.2876, -0.1290, -0.2773, -0.2527,  0.4482,  0.5534, -0.4047,  0.4932,\n",
       "        -0.3846, -0.2226, -0.2119, -0.3214,  0.0444, -0.1723,  0.0895,  0.2918,\n",
       "        -0.4092, -0.0892, -0.1816, -0.3662])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors[vocab[\"nice\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-mapping",
   "metadata": {},
   "source": [
    "Ok, but what about the words that are not in the vocabulary? How do we handle them?\n",
    "\n",
    "To all of them, there's a dedicated index: 0. However, maybe different vocabulary have a different standards, so you can just check it by running `vocab.unk_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "answering-collection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"asfgahsgf\"], vocab.unk_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-project",
   "metadata": {},
   "source": [
    "All right. I feel confident enough to say that the preprocessing part has been completed!\n",
    "\n",
    "Now we need to create the:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-appointment",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Yes, they are back. [Is it a good or a bad memory?](\"https://github.com/Strive-School/ai_mar21/blob/main/M5_Deep_Learning/D7/Custom%20DataLoader%20and%20Dataset.ipynb\")\n",
    "\n",
    "If you take a look at that notebook, you remember that to create a custom data loader you need to override some method of the `Dataset` class from `torch.utils.data`. Before doing so, let's define the steps we need to do while loading the data:\n",
    "\n",
    "- Receive as input a row from the dataframe that we have defined above, that contains two columns: \"star\" and \"review\"\n",
    "- we separate \"star\" from \"review\"\n",
    "- we preprocess the \"review\" columns by doing what we have so far (tokenization etc but excluding the embeddings for now)\n",
    "- as we have seen before, the possible stars are from 1 to 5. However, you should be advanced enough in Python to know that it starts from zero. So need to shift of the labels by 1 (e.g. if the stars are 4, the label will be 3).\n",
    "- Padding <- this is important, I will spend few words more in a few lines\n",
    "- Store a list containing the sequence of indices with the associated labels\n",
    "\n",
    "Then we need to override also the `__len__` and the `__getitem__`methods of the `Dataset` class.\n",
    "\n",
    "So few words about **padding**: not all the reviews have same length, so we need to find a solution for it. Why? Cause our Neural Network is waiting for input that are all of the same size! It needs to know how many weights it needs to initialize! \n",
    "\n",
    "There are several possibilities, but the easiest is to just set a cap with a `max_seq_len` parameter, so that all the reviews that are shorter than that length will be padded by using a vector associated with the padding index, and all the ones that are longer than `max_seq_len` will be just cut.\n",
    "\n",
    "Do you see problems? I actually don't see that much problems for it. I think that the sentiment of a comment can be seen already from the first words of the review.\n",
    "\n",
    "Ok, stop talking, more action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "selective-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    def __init__(self, df, max_seq_len=32): # df is the input df, max_seq_len is the max lenght allowed to a sentence before cutting or padding\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        counter = Counter()\n",
    "        train_iter = iter(df.review.values)\n",
    "        for text in train_iter:\n",
    "            counter.update(preprocessing(text))\n",
    "        self.vocab = Vocab(counter, min_freq=1)\n",
    "        self.vocab.load_vectors(\"fasttext.simple.300d\")\n",
    "        \n",
    "        label_pipeline = lambda x: int(x) - 1 # we need to preprocess the stars to start from 0\n",
    "        token2idx = lambda x: self.vocab[x] # Basically renaming functions to access them quickly\n",
    "        self.encode = lambda x: [token2idx(token) for token in preprocessing(x)]\n",
    "        self.pad = lambda x: x + (max_seq_len - len(x))*[token2idx(\"<pad>\")] # concatenating the original sentence with max_seq_len - len(x) padding indexes\n",
    "        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()] # here we are cutting to the max_seq_len and encoding\n",
    "        sequence, self.labels = zip(*[(sequence, label_pipeline(label)) for sequence, label in zip(sequences, df.star.tolist()) if sequence]) # not so much Pythonic I guess, a lot of list comprehension.\n",
    "        # If you get it fast, good, otherwise write your own version of it\n",
    "        self.sequences = [self.pad(sequence) for sequence in sequences]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        assert len(self.sequences[i]) == self.max_seq_len\n",
    "        return self.sequences[i], self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ready-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrainData(train_df, max_seq_len=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-repair",
   "metadata": {},
   "source": [
    "When we index dataset with a `dataset[index]` notation, we get the pair containing the padded sequence of indices with the associated label: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "massive-locking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10972,\n",
       "  12671,\n",
       "  311,\n",
       "  122,\n",
       "  12001,\n",
       "  221,\n",
       "  8525,\n",
       "  736,\n",
       "  199,\n",
       "  1569,\n",
       "  14,\n",
       "  2778,\n",
       "  1010,\n",
       "  1543,\n",
       "  3,\n",
       "  1113,\n",
       "  455,\n",
       "  3215,\n",
       "  2778,\n",
       "  457,\n",
       "  401,\n",
       "  1931,\n",
       "  5699,\n",
       "  120,\n",
       "  285,\n",
       "  194,\n",
       "  52,\n",
       "  29,\n",
       "  521,\n",
       "  1218,\n",
       "  1014,\n",
       "  481],\n",
       " 0)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "international-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2791,\n",
       " 1580,\n",
       " 204,\n",
       " 4,\n",
       " 392,\n",
       " 5572,\n",
       " 10415,\n",
       " 1897,\n",
       " 589,\n",
       " 388,\n",
       " 1004,\n",
       " 1646,\n",
       " 1049,\n",
       " 12157,\n",
       " 5642,\n",
       " 3719,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-sweden",
   "metadata": {},
   "source": [
    "What are the ones there? They are the product of the padding! \n",
    "\n",
    "What is the vector associated with the index 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "constitutional-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.vocab.vectors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-gothic",
   "metadata": {},
   "source": [
    "All zeros! Makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-domain",
   "metadata": {},
   "source": [
    "Storing into memory a lot of tensors containing all the embedded vectors, it can be very costly. This is why we load them by indexing with an integer. However, when we train our model, we need the embedded vectors!\n",
    "\n",
    "So let's define the `collate` function that will index our vocabulary only when it needs it!\n",
    "\n",
    "As argument it takes the batch (which will contains a `batch_size*max_seq_len` shape tensor) and the vectorizer. What is the vectorizer in our case? It's the vocabulary that we have built in the dataset object, so `dataset.vocab.vectors`. Indexing that vocabulary will retrieve the vector associated with that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "clean-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch, vectorizer=dataset.vocab.vectors):\n",
    "    inputs = torch.stack([torch.stack([vectorizer[token] for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch]) # Use long tensor to avoid unwanted rounding\n",
    "    return inputs, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-entrance",
   "metadata": {},
   "source": [
    "And now, we can use the `DataLoader` class as we did for images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "paperback-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "incident-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 300])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-sleeping",
   "metadata": {},
   "source": [
    "Ready to train? Following is a small model to *makes things to run on my computer*. You can expect to be kicked out if you come at the debrief with this model! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "third-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "emb_dim = 300\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim, hidden1=16, hidden2=16):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(max_seq_len*emb_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 5)\n",
    "        self.out = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1).float()))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bridal-commander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=9600, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=5, bias=True)\n",
       "  (out): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 32\n",
    "model = Classifier(MAX_SEQ_LEN, 300, 16, 16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "molecular-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "romantic-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sentences, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "suburban-error",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1856, 0.2146, 0.2034, 0.1972, 0.1992]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass through the network\n",
    "sentence_idx = 0\n",
    "sentences.resize_(16, 1, MAX_SEQ_LEN*emb_dim).shape\n",
    "log_ps = model.forward(sentences[sentence_idx,:])\n",
    "\n",
    "sentence = sentences[sentence_idx]\n",
    "torch.exp(log_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-prospect",
   "metadata": {},
   "source": [
    "We got 5 probabilities: one for each of the possible rating star!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "elementary-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0398\n",
      "\tIteration: 40\t Loss: 1.6229\n",
      "\tIteration: 80\t Loss: 1.6014\n",
      "\tIteration: 120\t Loss: 1.6091\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0346\n",
      "\tIteration: 40\t Loss: 1.4988\n",
      "\tIteration: 80\t Loss: 1.3547\n",
      "\tIteration: 120\t Loss: 1.5451\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0286\n",
      "\tIteration: 40\t Loss: 1.3033\n",
      "\tIteration: 80\t Loss: 0.8637\n",
      "\tIteration: 120\t Loss: 1.0787\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (sentences, labels) in enumerate(iter(train_loader)):\n",
    "\n",
    "        sentences.resize_(sentences.size()[0], 32* emb_dim)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(sentences)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-palestinian",
   "metadata": {},
   "source": [
    "Eventually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "hungry-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "likely-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets.AmazonReviewFull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-nicholas",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Create a real training process: use the train, val, test split for the dataset\n",
    "- Create a training loop that includes validation and test at the end\n",
    "    - You can borrow from your previous work, no need to write it from scratch\n",
    "- If you want to, feel free to change dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-chart",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('my-project-env': virtualenvwrapper)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "737e9587d30d88c29f1321911dcbdbc8762e4df40f7563cdfd1b93bad311ab30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}