{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n    <h2 align=\"center\" style=\"color:#01ff84\">Binary Clasification: Pytorch</h2>\n<div>",
   "metadata": {
    "cell_id": "00000-403e260e-c038-4863-ad91-0ea45f21c829",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Import the libraries",
   "metadata": {
    "cell_id": "00001-893257fe-3110-44df-b233-9934dd5facfe",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hjwSKXAO6w2j",
    "cell_id": "00002-d45c448c-0644-4303-8892-3e13d99f4cbf",
    "deepnote_cell_type": "code"
   },
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Till now you have worked with numpy to create and manage arrays. However, Pytorch has its own way to define arrays, or tensors in general, that are more convenient for computational efficiency:\n\nThe torch.tensor command has the same purpose of the np.array, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the dtype=torch.xxx command.\n\n",
   "metadata": {
    "id": "IQVubjP48NOr",
    "cell_id": "00003-7bb002d8-3b84-49ec-bd1d-b847517978bd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EbW3fZBQ8F_f",
    "cell_id": "00004-e76af023-8cd2-448a-a112-1d9dc75f9ec5",
    "deepnote_cell_type": "code"
   },
   "source": "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\ny = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\ntest_sample = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "You can inspect the size of the tensors:",
   "metadata": {
    "id": "ojuRtlUO9f2A",
    "cell_id": "00005-68828ed6-c5dc-4985-a35d-637a8cdfd260",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aByDVb-U9X8e",
    "outputId": "63978641-9d14-4c8d-ccfe-9374b6a0430f",
    "cell_id": "00006-d8935ed4-5949-4eee-ba21-f7dfeefac606",
    "deepnote_cell_type": "code"
   },
   "source": "print(X.size())\nprint(y.size())",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([3, 2])\ntorch.Size([3, 1])\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "If you already have a numpy array that you want to convert in tensor you can use:\n",
   "metadata": {
    "id": "6ysRjgTe9_1w",
    "cell_id": "00007-7393f3b2-2fb7-4909-a462-baa9d5283339",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41-_f5mY9hMP",
    "outputId": "10e9e305-e628-4994-9e44-0a6474612d0f",
    "cell_id": "00008-24b04516-e561-45e1-94f7-8f2bca8ea6a7",
    "deepnote_cell_type": "code"
   },
   "source": "np.random.seed(42)\nnumpy_array = np.array(np.random.rand(1000,10))\npytorch_tensor = torch.from_numpy(numpy_array)\nprint(pytorch_tensor.size())",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1000, 10])\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Pytorch has the advantage to be much more customizable and Keras, but this means that you have to manually define more things. \n\nIndeed, a neural network in Pytorch is a subclass of the nn.Module parent class.\n\n",
   "metadata": {
    "id": "_xxuR29T-Z5r",
    "cell_id": "00009-35d5d76d-cc1e-4fd6-b17b-7756ffb1f9c1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AkK6ULdl-Oiz",
    "cell_id": "00010-2f46bad0-2668-4275-81a9-6882f3d09bff",
    "deepnote_cell_type": "code"
   },
   "source": "class NeuralNetwork(nn.Module):\n    def __init__(self ):\n        super(NeuralNetwork, self).__init__()\n        pass",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The code above is how you define a neural network class using Pytorch. You can add some arguments to specify the input and the output sizes, or the number of neurons in the hidden layer:",
   "metadata": {
    "id": "otF4dBqSCpFn",
    "cell_id": "00011-ad95366e-4edd-45dd-aafd-e49da9fd5796",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4EhGXDx-_VMD",
    "cell_id": "00012-c2b09489-423a-4442-b33e-5b6e922e32ec",
    "deepnote_cell_type": "code"
   },
   "source": "class NeuralNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim, num_hidden):\n        super(NeuralNetwork, self).__init__()\n        pass\n",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The fully-connected layer in Pytorch (equivalent to the Dense layer in Keras) is given by the function nn.Linear() that takes as input the input shape and the number of neurons. We can define the layers as follows:",
   "metadata": {
    "id": "dKevxbyRHe52",
    "cell_id": "00013-b8160f1a-2e2a-4f3b-aae0-58df9d6891f0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WkvYry5987U8",
    "cell_id": "00014-8745f7d7-2b7e-4e8f-bbb7-bd1e34cd2d63",
    "deepnote_cell_type": "code"
   },
   "source": "class NeuralNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super().__init__()\n        self.linear1 = nn.Linear(input_dim, num_hidden)\n        self.sigmoid = nn.Sigmoid()\n        self.linear2 = nn.Linear(num_hidden, 1)",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We can instantiate a neural network as we do for a class (in the following example I will pass 10 as input_dim and 5 neurons for the hidden layer):",
   "metadata": {
    "id": "FBoLZeTJPy2A",
    "cell_id": "00015-1479c91d-1176-49e2-b06a-bcf5b1d45aae",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YiY6s5x5-Bgp",
    "cell_id": "00016-b97ec9d3-3d96-4a7d-9d28-f3d9e69b0883",
    "deepnote_cell_type": "code"
   },
   "source": "net = NeuralNetwork(10, 5)\n",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "In class you have seen that there are two main steps that are repeated over and over during the training process: the forward pass and backward pass. The forward is just making your input going through the network, doing the weights multiplication and so on as you have seen for the perceptron. While in Keras you have the fit() method that does both the step for you, in Pytorch, you have to implement the methods yourself:",
   "metadata": {
    "id": "D8hq_2sgP_9k",
    "cell_id": "00017-ceee7518-39b3-4f36-ae01-17ccfa01307a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kP5BLVDIQhpN",
    "cell_id": "00018-eef65f96-d76f-4dc2-a022-6a087a7e7744",
    "deepnote_cell_type": "code"
   },
   "source": "class NeuralNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super().__init__()\n        self.linear1 = nn.Linear(input_dim, num_hidden)\n        self.sigmoid = nn.Sigmoid()\n        self.linear2 = nn.Linear(num_hidden, 1)\n\n    def forward(self, x):\n        l1 = self.linear1(x)\n        activation = self.sigmoid(l1)\n        l2 = self.linear2(activation)\n        output = self.sigmoid(l2)\n        return output",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "As you can see in the code, each step in the forward pass is assigned to a variable, that is then passed to next layer!\n\n- The input x is passed in a linear layer\n- the output of that is assigned to the variable l1\n- l1 is activated using the sigmoid and assigned to the activation variable\n- the activation variable is passed to the second layer...\n...till the output that is returned!\n\nLet's create a sample to play with and make a forward pass calling the forward method:",
   "metadata": {
    "id": "NeK_lJYBQ_FL",
    "cell_id": "00019-86438028-f260-457b-a277-fd560d219949",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P4nz_QzF-J-1",
    "cell_id": "00020-6ee840f1-fcea-4112-9db7-2ddf55c78ee7",
    "deepnote_cell_type": "code"
   },
   "source": "sample = torch.from_numpy(np.array(np.random.rand(10), dtype=np.float32))",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvUBb-CH-n9_",
    "outputId": "20601a5c-7d92-4769-b383-7dc4e411c468",
    "cell_id": "00021-8705d2ce-67df-4c56-b211-5148fed5803a",
    "deepnote_cell_type": "code"
   },
   "source": "sample.size()",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10])"
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tA9E26bRQ60U",
    "cell_id": "00022-51d12790-6201-4de1-b4f8-e1dbbadbceeb",
    "deepnote_cell_type": "code"
   },
   "source": "net = NeuralNetwork(10, 5)",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xh5l6Nmd-c_8",
    "outputId": "e054b4d0-157f-47da-f9f3-a42cc8101f08",
    "cell_id": "00023-04fa7df7-e1a2-4051-8519-901bf29bc30a",
    "deepnote_cell_type": "code"
   },
   "source": "net.forward(sample)",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.4554], grad_fn=<SigmoidBackward>)"
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "As expected, you got a single value between 0 and 1 (the output of the sigmoid).\nNow that we have implemented the forward pass, we need to implement the backpropagation! Not from scratch, don't worry!\n\nFirst, we need to define both an optimizer and a loss function to use this training. I chose Adam as optimizer, since it is a common choice among the scientific community, and the Binary Cross Entropy loss since we are in the binary classification setting.",
   "metadata": {
    "id": "GbLWLKCkR6Co",
    "cell_id": "00024-31824020-af5a-4d1c-8190-1ba35c52b90b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agmgCwrLD15n",
    "outputId": "86478e80-fee7-4934-c9f4-d99a9e73a934",
    "cell_id": "00025-c4ae5997-bffc-41c7-a8f1-0d1d11cc69d3",
    "deepnote_cell_type": "code"
   },
   "source": "class NeuralNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super().__init__()\n        self.linear1 = nn.Linear(input_dim, num_hidden)\n        self.sigmoid = nn.Sigmoid()\n        self.linear2 = nn.Linear(num_hidden, 1)\n\n    def forward(self, x):\n        l1 = self.linear1(x)\n        activation = self.sigmoid(l1)\n        l2 = self.linear2(activation)\n        output = self.sigmoid(l2)\n        return output\n        \n\n#torch_fit(x_tensor, y_true_tensor, model=model, loss=loss, lr=0.1, num_epochs=30)\n\nloss = nn.BCELoss()\nmodel = NeuralNetwork(10, 5)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nx = np.random.rand(1000,10)\ny = np.random.randint(0, 2, 1000)\nx_tensor = torch.tensor(x).float()\ny_true_tensor = torch.tensor(y).float()\ny_true_tensor = y_true_tensor.view(1000,1) # view function is the same as reshape in numpy\ny_pred_tensor = model(x_tensor)\nloss_value = loss(y_pred_tensor, y_true_tensor)\nprint(f\"Initial loss: {loss_value.item():.2f}\")\n",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Initial loss: 0.72\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "and then wrap the training process in a unique function:",
   "metadata": {
    "id": "2w8riJwWHnYt",
    "cell_id": "00026-420e0d94-1f25-45af-8ec8-115e04203bf0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSo5PiJnHsdN",
    "outputId": "dfd616cb-8172-4e44-f60a-3daa27222abd",
    "cell_id": "00027-583d28bd-b532-4d46-bb6e-8383f633c402",
    "deepnote_cell_type": "code"
   },
   "source": "def torch_fit(x, y, model, loss, lr, num_epochs):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    for epoch in range(num_epochs):\n        optimizer.zero_grad()\n        y_pred_tensor = model(x_tensor)\n        loss_value = loss(y_pred_tensor, y_true_tensor)\n        print(f'Epoch {epoch}, loss {loss_value.item():.2f}')\n        loss_value.backward()\n        optimizer.step()\n    return model\n\nmodel = torch_fit(x_tensor, y_true_tensor, model=model, loss=loss, lr=0.1, num_epochs=100)\n",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 0, loss 0.61\nEpoch 1, loss 0.61\nEpoch 2, loss 0.61\nEpoch 3, loss 0.61\nEpoch 4, loss 0.61\nEpoch 5, loss 0.61\nEpoch 6, loss 0.61\nEpoch 7, loss 0.61\nEpoch 8, loss 0.61\nEpoch 9, loss 0.61\nEpoch 10, loss 0.61\nEpoch 11, loss 0.61\nEpoch 12, loss 0.61\nEpoch 13, loss 0.60\nEpoch 14, loss 0.60\nEpoch 15, loss 0.60\nEpoch 16, loss 0.60\nEpoch 17, loss 0.60\nEpoch 18, loss 0.60\nEpoch 19, loss 0.60\nEpoch 20, loss 0.60\nEpoch 21, loss 0.60\nEpoch 22, loss 0.60\nEpoch 23, loss 0.60\nEpoch 24, loss 0.60\nEpoch 25, loss 0.60\nEpoch 26, loss 0.60\nEpoch 27, loss 0.60\nEpoch 28, loss 0.60\nEpoch 29, loss 0.60\nEpoch 30, loss 0.60\nEpoch 31, loss 0.60\nEpoch 32, loss 0.60\nEpoch 33, loss 0.59\nEpoch 34, loss 0.59\nEpoch 35, loss 0.59\nEpoch 36, loss 0.59\nEpoch 37, loss 0.59\nEpoch 38, loss 0.59\nEpoch 39, loss 0.59\nEpoch 40, loss 0.59\nEpoch 41, loss 0.59\nEpoch 42, loss 0.59\nEpoch 43, loss 0.59\nEpoch 44, loss 0.59\nEpoch 45, loss 0.59\nEpoch 46, loss 0.59\nEpoch 47, loss 0.59\nEpoch 48, loss 0.59\nEpoch 49, loss 0.58\nEpoch 50, loss 0.58\nEpoch 51, loss 0.58\nEpoch 52, loss 0.58\nEpoch 53, loss 0.58\nEpoch 54, loss 0.58\nEpoch 55, loss 0.58\nEpoch 56, loss 0.58\nEpoch 57, loss 0.58\nEpoch 58, loss 0.58\nEpoch 59, loss 0.58\nEpoch 60, loss 0.58\nEpoch 61, loss 0.58\nEpoch 62, loss 0.58\nEpoch 63, loss 0.58\nEpoch 64, loss 0.57\nEpoch 65, loss 0.57\nEpoch 66, loss 0.57\nEpoch 67, loss 0.57\nEpoch 68, loss 0.57\nEpoch 69, loss 0.57\nEpoch 70, loss 0.57\nEpoch 71, loss 0.57\nEpoch 72, loss 0.57\nEpoch 73, loss 0.57\nEpoch 74, loss 0.57\nEpoch 75, loss 0.57\nEpoch 76, loss 0.57\nEpoch 77, loss 0.57\nEpoch 78, loss 0.56\nEpoch 79, loss 0.56\nEpoch 80, loss 0.56\nEpoch 81, loss 0.56\nEpoch 82, loss 0.56\nEpoch 83, loss 0.56\nEpoch 84, loss 0.56\nEpoch 85, loss 0.56\nEpoch 86, loss 0.56\nEpoch 87, loss 0.56\nEpoch 88, loss 0.56\nEpoch 89, loss 0.56\nEpoch 90, loss 0.56\nEpoch 91, loss 0.55\nEpoch 92, loss 0.55\nEpoch 93, loss 0.55\nEpoch 94, loss 0.55\nEpoch 95, loss 0.55\nEpoch 96, loss 0.55\nEpoch 97, loss 0.55\nEpoch 98, loss 0.55\nEpoch 99, loss 0.55\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Nice! You have trained your first model in Pytorch!\n\n",
   "metadata": {
    "id": "ijMUN-7yI5NE",
    "cell_id": "00028-f500dfef-9e49-4afe-b589-861438710484",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=796ba814-f632-4502-b0d7-7e2f8f9e546d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 1,
 "metadata": {
  "colab": {
   "name": "Pytorch Intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "deepnote_notebook_id": "3df797b8-124c-4317-9b2c-189429686748",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}